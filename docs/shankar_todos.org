** Developments

*** Argumentation workflow

**** Architecture
***** TODO identify all areas where changes would be necessary in bert code regarding own data
***** TODO perhaps focus only on task 1 instead of multi-task, or maybe that could the last priority
***** TODO possibly undo work tokenization via nltk and have it done in bert
***** TODO start off with bert seq2seq tagger in tensorflow, and advance application from there
***** TODO make easy data split for temporary model run, weakest point is the model
***** TODO add checkpoints and early stoppage to find better models in training, find ways to integrate bert into training procedure
***** possible to evaluate with accuracy metrics as well
***** split data into various sets and think of useful means of evaluating results, add various parameters such as window size for errors, perplexity and other useful parameters
***** perform single task first, and then multi task to check performance
***** consider non-transformer approach for character data due to GPU OOM issues -> perhaps adding more features to unknown words
***** try novel architectures for seq2seq task, egs. GRU, transformer, BERT pre-trained models
***** think of best unique tree structure classification, perhaps with argument connection distances
***** if working with three-way task, need to think of how to pass a gradient on non-existent examples -> perhaps some kind of negative sampling procedure

**** Code-specifc development
***** TODO add existing folder checks, creation if missing and trailing slash addition
***** TODO figure out pip local environment for earlier tensorflow version
***** TODO find out how to include fixed names into requirements.txt file such as tensorflow, despite no explicit call in script
***** fix slash error possibilities in path argument, check if directory exists to prevent later error
***** add log files and model folders like other ML projects, where detailed reconstruction information for models can be stored along with many performance metrics and example runs

**** Sequence encoding
***** TODO redefine padding length based on UNSC dataset paragraph or processing lengths
***** fix up data structure with different tasks later on, perhaps can merge all tasks into one, or keep multiple tasks
***** add data options with both cased and non-cased views 
***** need to split UNSC smaller speech segments or paragraphs to pass into pipeline
***** simple (task 1) -> 1: claim, 2: premise, 3: non-argument
***** tree (task 1) -> 1: claim, 2: aux claim connecting to same claim (behind), 3: premise connecting to claim, 4: aux premise connecting to same premise (behind), 5: non-argument
***** tree (task 2) -> distances to connective argument components which can help form tree

**** Domain debiasing
***** re-sampling procedure to re-train inputs with rare words more than common words
***** remove capital names and references to reduce bias
***** consider using special word embeddings and keep unmodified to retain word relationships
***** possibly add unknown token types eg. pos-tags, ner taggers, verb types, etc.
***** experiment specific entity/token masking to prevent domain-specific bias from training vocabulary
***** perhaps collapse all first, second and third-person pronouns to prevent self-referential bias 
***** add different classes in unknown vocabulary -> such as unknown noun, unknown adjective etc.

**** Documentation
***** fill up pydocstrings for publishable functions
***** redo colab notebook to clone and reset from master branch when publishing

**** Ideas to extrapolate
***** OOM issues for character-transformer model
***** ibm argumentation dataset
***** coreference resolution for tree structures
***** try genereous claims and premises creation and map via negative sampling to actual trees and redundant candidates
