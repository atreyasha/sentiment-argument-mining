** Developments 
   
*** Argumentation workflow

**** Training pipeline
***** save and load presentation in locations in github repo
***** fix f1 scoring issue where non-critical labels are macro-averaged, not necessary (perhaps do it in grid-search and normal training for all cases in order to prevent skewing of results)
***** where possible, flatten before applying dense, otherwise don't apply it
***** sync presentation and new bibtex file to github, make code better and clean up mess
***** consider how to deal with extra calls to corpus, perhaps better to merge all
***** add class f1s to grid searches later on and replace them in the meantime where possible
***** consider using aur or auc within models
***** TODO possibly create explicit validation set
***** TODO replace model names to follow ordering and sorting with batch norm or without
***** TODO make automated script to repeat tests for best performing n models to get statistical variations
***** TODO edit all log csvs to incorporate all relevant information 
***** TODO consider looking into aur/auc scores if that would help with evaluation
***** TODO add classification report jsons to logging pipelines, add them manually for other cases by reconstructing model and working with it
***** TODO check classification reports to better introspect models and their functionalities
***** TODO add grid-search json to help with choices defined on disk
***** TODO look at *run.ai* for accumulation optimzers and implement training generators -> can increase batch-size for grid-search
***** TODO work on task 1 and observe how multi-task setting could improve both tasks, use *adjacency matrix* for second task
***** TODO update models in logs to have 0 index for cnn and lstm *and* with/without class weights
***** try out different val metric
***** possible script for continue training if patience not triggered; look up model reconstruction by adding custom objects
***** when converting to graph, mask out N to zero in adjacency matrix

**** Sequence encoding
***** TODO improve multitask data processing pipeline with task specification and complete json corpus with argument structure as matrix
***** TODO improve splits in next runs with more thought put behind into distribution of splits
***** look into argument structure and ensure all arguments are present in same paragraph
***** find shorter sequence candidates in UNSC corpus for testing out model 

**** Domain debiasing
***** TODO remove capital names and references to reduce bias
***** increase sequence length by using accumulation to allow more data to feed into network 
     
**** Code-specifc development
***** TODO find out how to include fixed names into requirements.txt file such as tensorflow, despite no explicit call in script, figure out pip local environment and how to fix this for future development
***** TODO update all readmes and pydocstrings, check unused imports and code health in general
***** add appropriate citations for code, review to make sure this is done correctly
***** add existing folder checks, creation if missing and trailing slash addition

**** Task construction
***** task 1 -> 1: claim, 2: premise, 3: non-argument
***** task 2 (dependent on task 1) -> form argumentation structure with adjacency matrix, multiply input from task 1 by row
     
**** Story for presentation
***** clause extraction did not show reliable results with benepar and hard to process
***** mention using various [SEP] indicators for flipping sentences (need some more backup information for this process)
***** mention memory issues related to bert, therefore trying albert with single gpu -> talk about differences between albert and bert
***** also shorter sequence length due to memory issues, makes for better toy examples

**** Ideas to explore
***** ibm argumentation dataset
***** coreference resolution for tree structures
***** try genereous claims and premises creation and map via negative sampling to actual trees and redundant candidates
